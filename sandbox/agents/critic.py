"""
Agente Cr√≠tico (Reflection Pattern)

Responsable de evaluar la l√≥gica y calidad del c√≥digo generado por el Coder
antes de pasar a la simulaci√≥n. Verifica alineaci√≥n con la tarea y l√≥gica de negocio.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from typing import Dict, Any
from langchain_ollama import ChatOllama

from config.settings import (
    OLLAMA_BASE_URL,
    MODEL_REASONING,
    MODEL_TEMPERATURE_REASONING
)
from utils.state import AgentState, add_audit_entry
from utils.logging_utils import update_agent_status, log_message

def critic_node(state: AgentState) -> Dict[str, Any]:
    """
    Nodo del agente cr√≠tico para LangGraph.
    Eval√∫a el c√≥digo generado y decide si es apto para simulaci√≥n.
    
    Args:
        state: Estado actual del sistema
        
    Returns:
        Actualizaci√≥n del estado con feedback del cr√≠tico
    """
    print("\n" + "="*80)
    print("üßê AGENTE CR√çTICO ACTIVADO")
    print("="*80)
    
    task = state['task']
    code = state.get('code_snippet', '')
    iteration = state.get('iteration', 0)
    
    update_agent_status("Critic", "running", "Evaluando l√≥gica del c√≥digo...")
    log_message("Critic", f"Iniciando evaluaci√≥n de c√≥digo para: {task}")
    
    if not code:
        return {
            'critic_approved': False,
            'critique': "No hay c√≥digo para evaluar.",
            **add_audit_entry(state, "critic", "evaluation_failed", {'reason': "no_code"})
        }

    try:
        llm = ChatOllama(
            model=MODEL_REASONING,
            temperature=MODEL_TEMPERATURE_REASONING,
            base_url=OLLAMA_BASE_URL
        )
        
        prompt = f"""
Act√∫a como un Revisor de C√≥digo Experto en NS-3 y Redes.
Tu objetivo es encontrar ERRORES L√ìGICOS o DE ALINEACI√ìN con la tarea. NO te preocupes por errores de sintaxis (eso lo hace el compilador).

**TAREA ORIGINAL:**
{task}

**C√ìDIGO GENERADO:**
```python
{code[:4000]}  # Truncado para evitar contexto excesivo si es muy largo
```

**CRITERIOS DE EVALUACI√ìN:**
1. ¬øEl c√≥digo implementa el protocolo solicitado? (Ej: Si pide AODV, ¬øusa AODV?)
2. ¬øLa topolog√≠a y movilidad coinciden con lo pedido?
3. ¬øSe est√°n recolectando las m√©tricas necesarias?
4. ¬øHay l√≥gica "tonta" o placeholders obvios?

**FORMATO DE RESPUESTA:**
Responde EXACTAMENTE con este formato JSON:
{{
    "approved": true/false,
    "critique": "Explicaci√≥n breve del problema (si approved=false) o 'Aprobado' (si approved=true)"
}}
"""
        
        print("  ü§î Analizando l√≥gica y alineaci√≥n...")
        response = llm.invoke(prompt)
        content = response.content.strip()
        
        # Intentar parsear JSON (o buscarlo en el texto)
        import json
        import re
        
        approved = False
        critique = "Error parseando respuesta del cr√≠tico"
        
        # Buscar bloque JSON
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(0))
                approved = data.get('approved', False)
                critique = data.get('critique', "Sin comentarios")
            except:
                # Fallback si el JSON est√° mal formado
                if "true" in content.lower() and "approved" in content.lower():
                    approved = True
                    critique = "Aprobado (Fallback parse)"
                else:
                    critique = content[:200]
        else:
             # Fallback si no hay JSON
            # Fallback si no hay JSON
            content_lower = content.lower()
            if "approved" in content_lower and "rejected" not in content_lower:
                approved = True
                critique = "Aprobado (No JSON)"
            elif "true" in content_lower and "false" not in content_lower:
                approved = True
                critique = "Aprobado (No JSON)"
            else:
                critique = content[:200]

        
        if approved:
            print("  ‚úÖ C√≥digo APROBADO por el Cr√≠tico")
            log_message("Critic", "C√≥digo aprobado")
            return {
                'critic_approved': True,
                'critique': critique,
                **add_audit_entry(state, "critic", "approved", {'critique': critique})
            }
        else:
            print(f"  ‚ùå C√≥digo RECHAZADO: {critique}")
            log_message("Critic", f"C√≥digo rechazado: {critique}", level="WARNING")
            return {
                'critic_approved': False,
                'critique': critique,
                # Incrementar iteraci√≥n aqu√≠ podr√≠a ser opcional, pero mejor dejar que el supervisor decida
                # o que el coder incremente al reintentar.
                # Por ahora, pasamos el feedback.
                **add_audit_entry(state, "critic", "rejected", {'critique': critique})
            }

    except Exception as e:
        print(f"  ‚ö†Ô∏è Error en cr√≠tico: {e}")
        log_message("Critic", f"Error ejecutando cr√≠tico: {e}", level="ERROR")
        # En caso de error del cr√≠tico, aprobamos por defecto para no bloquear
        return {
            'critic_approved': True, 
            'critique': "Error en cr√≠tico, aprobado por defecto.",
            **add_audit_entry(state, "critic", "error_bypass", {'error': str(e)})
        }
