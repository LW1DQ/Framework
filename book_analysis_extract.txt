

=== Chapter 9 ===

--- Page 21 ---
ABOUT THIS BOOK xix
of LLM agents. This chapter explores how retrieval mechanisms can serve as
both a source of knowledge by processing ingested files, and of memory, allow-
ing agents to recall previous interactions or events.
 Chapter 9, “Mastering agent prompts with prompt flow”—Prompt engineering is cen-
tral to an agent’s success. This chapter introduces prompt flow, a tool from Mic-
rosoft that helps automate the testing and evaluation of prompts, enabling
more robust and effective agentic behavior.
 Chapter 10, “Agent reasoning and evaluation”—Reasoning is crucial to solving
problems intelligently. In this chapter, we explore various reasoning techniques,
such as chain of thought (CoT), and show how agents can evaluate reasoning
strategies even during inference, improving their capacity to solve problems
autonomously.
 Chapter 11, “Agent planning and feedback”—Planning is perhaps an agent’s most crit-
ical skill in achieving its goals. We discuss how agents can incorporate planning to
navigate complex tasks and how feedback loops can be used to refine those plans.
The chapter concludes by integrating all the key components—actions, memory
and knowledge, reasoning, evaluation, planning, and feedback—into practical
examples of agentic systems that solve real-world problems.
About the code
The code for this book is spread across several open source projects, many of which
are hosted by me or by other organizations in GitHub repositories. Throughout this
book, I strive to make the content as accessible as possible, taking a low-code approach
to help you focus on core concepts. Many chapters demonstrate how simple prompts
can generate meaningful code, showcasing the power of AI-assisted development.
Additionally, you’ll find a variety of assistant profiles and multi-agent systems that
demonstrate how to solve real-world problems using generated code. These examples
are meant to inspire, guide, and empower you to explore what is possible with AI
agents. I am deeply grateful to the many contributors and the community members
who have collaborated on these projects, and I encourage you to explore the reposito-
ries, experiment with the code, and adapt it to your own needs. This book is a testa-
ment to the power of collaboration and the incredible things we can achieve together.
This book contains many examples of source code both in numbered listings and in
line with normal text. In both cases, source code is formatted in a fixed-width font
like this to separate it from ordinary text. Sometimes, some of the code is typeset in
bold to highlight code that has changed from previous steps in the chapter, such as
when a feature is added to an existing line of code. In many cases, the original source
code has been reformatted; we’ve added line breaks and reworked indentation to
accommodate the available page space in the book. In some cases, even this wasn’t
enough, and listings include line-continuation markers (➥). Additionally, comments in
the source code have often been removed from the listings when the code is described
--- Page 22 ---
xx ABOUT THIS BOOK
in the text. Code annotations accompany many of the listings, highlighting important
concepts.
You can get executable snippets of code from the liveBook (online) version of this
book at https://livebook.manning.com/book/ai-agents-in-action. The complete code
for the examples in the book is available for download from the Manning website at
www.manning.com/books/ai-agents-in-action. In addition, the code developed for this
book has been placed in three GitHub repositories that are all publicly accessible:
 GPT-Agents (the original book title), at https://github.com/cxbxmxcx/GPT-
Agents, holds the code for several examples demonstrated in the chapters.
 GPT Assistants Playground, at https://github.com/cxbxmxcx/GPTAssistants
Playground, is an entire platform and tool dedicated to building OpenAI GPT
assistants with a helpful web user interface and plenty of tools to develop auton-
omous agent systems.
 Nexus, at https://github.com/cxbxmxcx/Nexus, is an example of a web-based
agentic tool that can help you create agentic systems and demonstrate various
code challenges.
liveBook discussion forum
Purchase of AI Agents in Action includes free access to liveBook, Manning’s online
reading platform. Using liveBook’s exclusive discussion features, you can attach com-
ments to the book globally or to specific sections or paragraphs. It’s a snap to make
notes for yourself, ask and answer technical questions, and receive help from the
author and other users. To access the forum, go to https://livebook.manning.com/
book/ai-agents-in-action/discussion. You can also learn more about Manning’s forums
and the rules of conduct at https://livebook.manning.com/discussion.
Manning’s commitment to our readers is to provide a venue where a meaningful
dialogue between individual readers and between readers and the author can take
place. It isn’t a commitment to any specific amount of participation on the part of the
author, whose contribution to the forum remains voluntary (and unpaid). We suggest
you try asking the him challenging questions lest his interest stray! The forum and the
archives of previous discussions will be accessible from the publisher’s website as long
as the book is in print.
--- Page 23 ---
about the author
MICHEAL LANHAM is a distinguished software and technology
innovator with more than two decades of experience in the
industry. He has an extensive background in developing various
software applications across several domains, such as gaming,
graphics, web development, desktop engineering, AI, GIS, oil
and gas geoscience/geomechanics, and machine learning.
Micheal began by pioneering work in integrating neural net-
works and evolutionary algorithms into game development,
which began around the turn of the millennium. He has
authored multiple influential books exploring deep learning, game development,
and augmented reality, including Evolutionary Deep Learning (Manning, 2023) and
Augmented Reality Game Development (Packt Publishing, 2017). He has contributed to
the tech community via publications with many significant tech publishers, including
Manning. Micheal resides in Calgary, Alberta, Canada, with his large family, whom he
enjoys cooking for.
xxi
--- Page 24 ---
about the cover illustration
The figure on the cover of AI Agents in Action is “Clémentinien,” taken from Balthasar
Hacquet’s Illustrations de L’Illyrie et la Dalmatie, published in 1815.
In those days, it was easy to identify where people lived and what their trade or sta-
tion in life was just by their dress. Manning celebrates the inventiveness and initiative
of the computer business with book covers based on the rich diversity of regional cul-
ture centuries ago, brought back to life by pictures from collections such as this one.
xxii
--- Page 25 ---
Introduction to agents
and their world
This chapter covers
 Defining the concept of agents
 Differentiating the components of an
agent
 Analyzing the rise of the agent era:
Why agents?
 Peeling back the AI interface
 Navigating the agent landscape
The agent isn’t a new concept in machine learning and artificial intelligence (AI).
In reinforcement learning, for instance, the word agent denotes an active decision-
making and learning intelligence. In other areas, the word agent aligns more with
an automated application or software that does something on your behalf.
1.1 Defining agents
You can consult any online dictionary to find the definition of an agent. The Mer-
riam-Webster Dictionary defines it this way (www.merriam-webster.com/dictionary/
agent):
1
--- Page 26 ---
2 CHAPTER 1 Introduction to agents and their world
 One that acts or exerts power
 Something that produces or can produce an effect
 A means or instrument by which a guiding intelligence achieves a result
The word agent in our journey to build powerful agents in this book uses this dictio-
nary definition. That also means the term assistant will be synonymous with agent.
Tools like OpenAI’s GPT Assistants will also fall under the AI agent blanket. OpenAI
avoids the word agent because of the history of machine learning, where an agent is
self-deciding and autonomous.
Figure 1.1 shows four cases where a user may interact with a large language model
(LLM) directly or through an agent/assistant proxy, an agent/assistant, or an autono-
mous agent. These four use cases are highlighted in more detail in this list:
 Direct user interaction—If you used earlier versions of ChatGPT, you experienced
direct interaction with the LLM. There is no proxy agent or other assistant
interjecting on your behalf.
 Agent/assistant proxy—If you’ve used Dall-E 3 through ChatGPT, then you’ve expe-
rienced a proxy agent interaction. In this use case, an LLM interjects your
requests and reformulates them in a format better designed for the task. For
example, for image generation, ChatGPT better formulates the prompt. A proxy
agent is an everyday use case to assist users with unfamiliar tasks or models.
 Agent/assistant—If you’ve ever used a ChatGPT plugin or GPT assistant, then
you’ve experienced this use case. In this case, the LLM is aware of the plugin
or assistant functions and prepares to make calls to this plugin/function.
However, before making a call, the LLM requires user approval. If approved,
the plugin or function is executed, and the results are returned to the LLM.
The LLM then wraps this response in natural language and returns it to
theuser.
 Autonomous agent—In this use case, the agent interprets the user’s request, con-
structs a plan, and identifies decision points. From this, it executes the steps in
the plan and makes the required decisions independently. The agent may
request user feedback after certain milestone tasks, but it’s often given free rein
to explore and learn if possible. This agent poses the most ethical and safety
concerns, which we’ll explore later.
Figure 1.1 demonstrates the use cases for a single flow of actions on an LLM using a
single agent. For more complex problems, we often break agents into profiles or per-
sonas. Each agent profile is given a specific task and executes that task with specialized
tools and knowledge.
Multi-agent systems are agent profiles that work together in various configurations to
solve a problem. Figure 1.2 demonstrates an example of a multi-agent system using
three agents: a controller or proxy and two profile agents as workers controlled by the
proxy. The coder profile on the left writes the code the user requests; on the right is a
--- Page 27 ---
1.1 Defining agents 3
No agent or assistant Agent/assistant proxy for Agent/assistant acting on Autonomous agent making
direct connection to LLM image generator behalf of user decisions on behalf of user
Filter my emails by
Please explain the Show an illustration What is the temperature importance and notify
definition of agent. of an agent. in Calgary today? me of the top 5 most
important emails.
Large language model Large language model Large language model Large language model
(ChatGPT) (ChatGPT) (ChatGPT) (ChatGPT)
"An image of a female LLM identifies an external LLM identifies an external
LLM:The definition secret agent of Hispanic functionAPI to call and functionAPIto call and
of agent is... descent in a nighttime parameters to connect parameters to connect
urban setting. . . to a weather service. to an email service.
Decision step
Asks user if it’s okay
to execute the function
on their behalf.
LLM reads
User confirms
Image generation model and sorts emails by
execution okay.
(DALL-E 3) what it deems to be
important.
Executes the function
and returns weather
information.
LLM reformul.ates
Notifies the user of
weather information and
important emails.
responds to the user.
Figure 1.1 The differences between the LLM interactions from direct action compared to using proxy agents,
agents, and autonomous agents
tester profile designed to write unit tests. These agents work and communicate
together until they are happy with the code and then pass it on to the user.
Figure 1.2 shows one of the possibly infinite agent configurations. (In chapter 4,
we’ll explore Microsoft’s open source platform, AutoGen, which supports multiple
configurations for employing multi-agent systems.)
Multi-agent systems can work autonomously but may also function guided entirely
by human feedback. The benefits of using multiple agents are like those of a single
--- Page 28 ---
4 CHAPTER 1 Introduction to agents and their world
User query Answer
Feature
reque
Cst
ode
CodeUnit
tests
Controller Repeat until the
The controller agent code and tests
can execute code on work as expected.
the user’s behalf.
Worker agents
interact with
Large Language Model Large Language Model
the LLM.
Coder Tester
Figure 1.2 In this example of a multi-agent system, the controller or agent proxy communicates directly with
the user. Two agents—a coder and a tester—work in the background to create code and write unit tests to test
the code.
agent but often magnified. Where a single agent typically specializes in a single task,
multi-agent systems can tackle multiple tasks in parallel. Multiple agents can also pro-
vide feedback and evaluation, reducing errors when completing assignments.
As we can see, an AI agent or agent system can be assembled in multiple ways.
However, an agent itself can also be assembled using multiple components. In the
next section, we’ll cover topics ranging from an agent’s profile to the actions it may
perform, as well as memory and planning.
1.2 Understanding the component systems of an agent
Agents can be complex units composed of multiple component systems. These com-
ponents are the tools the agent employs to help it complete its goal or assigned tasks
and even create new ones. Components may be simple or complex systems, typically
split into five categories.
Figure 1.3 describes the major categories of components a single-agent system may
incorporate. Each element will have subtypes that can define the component’s type,
--- Page 29 ---
1.2 Understanding the component systems of an agent 5
structure, and use. At the core of all agents is the profile and persona; extending from
that are the systems and functions that enhance the agent.
Actions andTool Use Profile and Persona Memory and Knowledge
Memory and knowledge provide
Actions represent a function/ added context to the agent for a
tool an agent can use. specific request or task.
Reasoning and Evaluation Planning and Feedback
Apersona represents the agent’s
main role or function, typically
defined in a system prompt.The
Reasoning and evaluation
profile describes the entire agent Planning and feedback
ground the agent and empower
system. allow the agent to learn and
it to make better decisions.
improve on task completion.
Figure 1.3 The five main components of a single-agent system (image generated through DALL-E 3)
The agent profile and persona shown in figure 1.4 represent the base description of
the agent. The persona—often called the system prompt—guides an agent to complete
tasks, learn how to respond, and other nuances. It includes elements such as the back-
ground (e.g., coder, writer) and demographics, and it can be generated through
methods such as handcrafting, LLM assistance, or data-driven techniques, including
evolutionary algorithms.
We’ll explore how to create effective and specific agent profiles/personas through
techniques such as rubrics and grounding. In addition, we’ll explain the aspects of
human-formulated versus AI-formulated (LLM) profiles, including innovative tech-
niques using data and evolutionary algorithms to build profiles.
--- Page 30 ---
6 CHAPTER 1 Introduction to agents and their world
Agent persona: We’ll understand how
to clearly define the persona, specifying
their role and characteristics to guide
the agent effectively.
Profile and Persona
Agent role and demographics: We’ll
see how relevant demographic and role
details can provide agent context, such
as age, gender, or background, for a
more relevant interaction.
Profile Contents
Persona: Role, i.e., coder or tester Human vs. AI assistance for persona
Demographics: Sex, age, background generation: We’ll highlight the role
of human involvement in persona
Profile Generation generation, whether it’s entirely
Handcrafted: Manually designed by human driven or assisted by LLMs
humans or other agents.
LLM generated: Directed by human
prompts
Data generated: Constructed from Innovative persona techniques:
data personas Prompts generated through data
or other novel approaches such as
evolutionary algorithms to enhance
agent capabilities.
Figure 1.4 An in-depth look at how we’ll explore creating agent profiles
NOTE The agent or assistant profile is composed of elements, including the
persona. It may be helpful to think of profiles describing the work the agent/
assistant will perform and the tools it needs.
Figure 1.5 demonstrates the component actions and tool use in the context of agents
involving activities directed toward task completion or acquiring information. These
actions can be categorized into task completion, exploration, and communication,
with varying levels of effect on the agent’s environment and internal states. Actions
can be generated manually, through memory recollection, or by following predefined
plans, influencing the agent’s behavior and enhancing learning.
Understanding the action target helps us define clear objectives for task comple-
tion, exploration, or communication. Recognizing the action effect reveals how actions
influence task outcomes, the agent’s environment, and its internal states, contributing
to efficient decision making. Lastly, grasping action generation methods equips us
with the knowledge to create actions manually, recall them from memory, or follow
predefined plans, enhancing our ability to effectively shape agent behavior and learn-
ing processes.
Figure 1.6 shows the component knowledge and memory in more detail. Agents
use knowledge and memory to annotate context with the most pertinent information
while limiting the number of tokens used. Knowledge and memory structures can be
unified, where both subsets follow a single structure or hybrid structure involving a
mix of different retrieval forms. Knowledge and memory formats can vary widely from
--- Page 31 ---
1.2 Understanding the component systems of an agent 7
Action and Tool Use
Action targets: We’ll learn the importance
of defining action targets, whether for task
completion, exploration, or communication,
to clarify the agent’s objectives.
Action Target
Action space and impact: We’ll learn the Semantic or native functions
significance of understanding how actions
affect task completion and their effect on
the agent’s environment, internal states, Tools, self-knowledge, other agents
and self-knowledge.
Environments, new actions, internal
Action generation methods: We’ll see the states, other agents
various ways actions can be generated, such
as manual creation, memory recollection,
or plan following, to illustrate the diversity Manual, memory recollection, plan
of agent behaviors. following
Figure 1.5 The aspects of agent actions we’ll explore in this book
language (e.g., PDF documents) to databases (relational, object, or document) and
embeddings, simplifying semantic similarity search through vector representations or
even simple lists serving as agent memories.
Retrieval structure variety: We’ll learn
Memory and Knowledge
about the diverse memory structures
agents can employ, including unified and
hybrid approaches, enablingflexibility in
information storage.
Retrieval formats: We’ll explore the
Retrieval Structure
various data sources for memory, such
•Unified
as language (e.g., PDF documents),
•Hybrid
databases (relational, object, or
Retrieval Formats
document), and embeddings, offering a
•Language
rich pool of information to draw upon.
•Databases
•Embeddings
•Lists Semantic similarity: We’ll learn how
Retrieval Operation embeddings enable semantic similarity
•Augmentation searches, facilitating efficient retrieval of
•Semantic Extraction relevant data and enhancing the agent’s
•Compression decision-making capabilities.
Figure 1.6 Exploring the role and use of agent memory and knowledge
Figure 1.7 shows the reasoning and evaluation component of an agent system. Research
and practical applications have shown that LLMs/agents can effectively reason. Rea-
soning and evaluation systems annotate an agent’s workflow by providing an ability to
think through problems and evaluate solutions.
--- Page 32 ---
8 CHAPTER 1 Introduction to agents and their world
Reasoning and Evaluation
Reasoning enables the agent
to self-reflect and internally
Reasoning reason out the completion
•Zero-shot prompting of a task or tasks.
•One-shot prompting
•Few-shot prompting
•Chain of thought
•Tree of thought
Evaluation provides the basis
•Skeleton of thought
for an agent’s self-reflection
Evaluation on working through and upon
•Self-consistency task completion.
•Prompt chaining
Figure 1.7 The reasoning and evaluation component and details
Figure 1.8 shows the component agent planning/feedback and its role in organizing
tasks to achieve higher-level goals. It can be categorized into these two approaches:
 Planning without feedback—Autonomous agents make decisions independently.
 Planning with feedback—Monitoring and modifying plans is based on various
sources of input, including environmental changes and direct human feedback.
Planning and Feedback
We’ll look at various planning Planning without feedback
(autonomous)
strategies with and without
•Basic planning
feedback—from basic and
•Automatic reasoning with
sequential planners to automatic tool use
tool use with reasoning. •Sequential planning
Planning with feedback
•Environmental feedback
Feedback may come from a variety •Human feedback
of sources, such as environmental, •LLM feedback
human, and an LLM via various •Adaptive constructive
constructive feedback patterns. feedback
Figure 1.8 Exploring the role of agent planning and reasoning
Within planning, agents may employ single-path reasoning, sequential reasoning through
each step of a task, or multipath reasoning to explore multiple strategies and save the
--- Page 33 ---
1.3 Examining the rise of the agent era: Why agents? 9
efficient ones for future use. External planners, which can be code or other agent sys-
tems, may also play a role in orchestrating plans.
Any of our previous agent types—the proxy agent/assistant, agent/assistant, or
autonomous agent—may use some or all of these components. Even the planning
component has a role outside of the autonomous agent and can effectively empower
even the regular agent.
1.3 Examining the rise of the agent era: Why agents?
AI agents and assistants have quickly moved from the main commodity in AI research
to mainstream software development. An ever-growing list of tools and platforms assist
in the construction and empowerment of agents. To an outsider, it may all seem like
hype intended to inflate the value of some cool but overrated technology.
During the first few months after ChatGPT’s initial release, a new discipline called
prompt engineering was formed: users found that using various techniques and patterns
in their prompts allowed them to generate better and more consistent output. How-
ever, users also realized that prompt engineering could only go so far.
Prompt engineering is still an excellent way to interact directly with LLMs such as
ChatGPT. Over time, many users discovered that effective prompting required iteration,
reflection, and more iteration. The first agent systems, such as AutoGPT, emerged from
these discoveries, capturing the community’s attention.
Figure 1.9 shows the original design of AutoGPT, one of the first autonomous
agent systems. The agent is designed to iterate a planned sequence of tasks that it
defines by looking at the user’s goal. Through each task iteration of steps, the agent
evaluates the goal and determines if the task is complete. If the task isn’t complete, the
agent may replan the steps and update the plan based on new knowledge or human
feedback.
AutoGPT became the first example to demonstrate the power of using task plan-
ning and iteration with LLM models. From this and in tandem, other agent systems
and frameworks exploded into the community using similar planning and task itera-
tion systems. It’s generally accepted that planning, iteration, and repetition are the
best processes for solving complex and multifaceted goals for an LLM.
However, autonomous agent systems require trust in the agent decision-making
process, the guardrails/evaluation system, and the goal definition. Trust is also some-
thing that is acquired over time. Our lack of trust stems from our lack of understand-
ing of an autonomous agent’s capabilities.
NOTE Artificial general intelligence (AGI) is a form of intelligence that
can learn to accomplish any task a human can. Many practitioners in this
new world of AI believe an AGI using autonomous agent systems is an
attainable goal.
For this reason, many of the mainstream and production-ready agent tools aren’t auton-
omous. However, they still provide a significant benefit in managing and automating
--- Page 34 ---
10 CHAPTER 1 Introduction to agents and their world
AutonomousAI Mechanism
User sets the main
objective goal
Setting Goals
Goals complete
Goals not complete
The agent can be
Defining and
SequencingTasks set to ask for Evaluation
permission for
every task or for
every x number
of tasks.
The agent evaluates if
the goal is complete
after every task
The agent iteration.
plans out the
sequence of
tasks to
undertake.
Task Execution
Agent executes
tasks. The agent could write code to
perform other tasks as needed.
AI Large Language
Internet OtherTools
Models (GPT-4)
Figure 1.9 The original design of the AutoGPT agent system
tasks using GPTs (LLMs). Therefore, as our goal in this book is to understand all agent
forms, many more practical applications will be driven by non-autonomous agents.
Agents and agent tools are only the top layer of a new software application devel-
opment paradigm. We’ll look at this new paradigm in the next section.
--- Page 35 ---
1.4 Peeling back the AI interface 11
1.4 Peeling back the AI interface
The AI agent paradigm is not only a shift in how we work with LLMs but is also per-
ceived as a shift in how we develop software and handle data. Software and data will
no longer be interfaced using user interfaces (UIs), application programming inter-
faces (APIs), and specialized query languages such as SQL. Instead, they will be
designed to be interfaced using natural language.
Figure 1.10 shows a high-level snapshot of what this new architecture may look like
and what role AI agents play. Data, software, and applications adapt to support seman-
tic, natural language interfaces. These AI interfaces allow agents to collect data and
interact with software applications, even other agents or agent applications. This rep-
resents a new shift in how we interact with software and applications.
Please create a report of
last year’s sales.
Planning:Agent takes the goal and breaks into tasks. Agent interface layer (natural language)
1. Collect data.
2.Annotate data. 4.Agent presents the report.
3. Format data, and create report visualizations.
4. Present report.
All communication via natural language
GPTdata layer GPTfunctions GPTtools
APIs,
web browsing,
Database search, etc. External agents
1. Query database using natural 2.Annotate data by calling semantic 3. External agent formats data and
language. functions using natural language. may write code to generate visuals.
Figure 1.10 A vision of how agents will interact with software systems
An AI interface is a collection of functions, tools, and data layers that expose data and
applications by natural language. In the past, the word semantic has been heavily

=== Chapter 8 ===

--- Page 80 ---
56 CHAPTER 3 Engaging GPT assistants
3.4 Extending an assistant’s knowledge using file uploads
If you’ve engaged with LLMs, you likely have heard about the retrieval augmented
generation (RAG) pattern. Chapter 8 will explore RAG in detail for the application of
both knowledge and memory. Detailed knowledge of RAG isn’t required to use the
file upload capability, but if you need some foundation, check out that chapter.
The GPT Assistants platform provides a knowledge capability called file uploads,
which allows you to populate the GPT with a static knowledge base about anything in
various formats. As of writing, the GPT Assistants platform allows you to upload up to
512 MB of documents. In the next two exercises, we’ll look at two different GPTs
designed to assist users with consuming books.
3.4.1 Building the Calculus Made Easy GPT
Books and written knowledge will always be the backbone of our knowledge base. But
reading text is a full-time concerted effort many people don’t have time for. Audio-
books made consuming books again accessible; you could listen while multitasking,
but not all books transitioned well to audio.
Enter the world of AI and intelligent assistants. With GPTs, we can create an inter-
active experience between the reader and the book. No longer is the reader forced to
consume a book page by page but rather as a whole.
To demonstrate this concept, we’ll build a GPT based on a classic math text called
Calculus Made Easy, by Silvanus P. Thompson. The book is freely available through the
Gutenberg Press website. While it’s more than a hundred years old, it still provides a
solid material background.
NOTE If you’re serious about learning calculus but this assistant is still too
advanced, check out a great book by Clifford A. Pickover called Calculus and
Pizza. It’s a great book for learning calculus or just to get an excellent
refresher. You could also try making your Calculus and Pizza assistant if you
have an eBook version. Unfortunately, copyright laws would prevent you from
publishing this GPT without permission.
Open ChatGPT, go to My GPTs, create a new GPT, click the Configure tab, and then
upload the file, as shown in figure 3.10. Upload the book from the chapter’s source
code folder: chapter _03/calculus_made_easy.pdf. This will add the book to the
GPT’s knowledge.
Scroll up and add the instructions shown in listing 3.16. The initial preamble text
was generated by conversing with the GPT Builder. After updating the preamble text,
a personality was added by asking ChatGPT for famous mathematicians. Then, finally,
rules were added to provide additional guidance to the GPT on what explicit out-
comes we want.
--- Page 81 ---
3.4 Extending an assistant’s knowledge using file uploads 57
Addingfiles is considered giving your
assistant additional knowledge.
Use the Uploadfiles button to add
various sources of static
knowledge for the assistant.
Be sure to enable Code Interpreter
so the assistant can demonstrate
concepts.
Figure 3.10 Adding files to the assistant’s knowledge
Listing3.16 Instructions for Calculus Made Easy GPT
This GPT is designed to be an expert teacher and mentor
of calculus based on the book 'Calculus Made Easy' by
Silvanus Thompson. A copy of the book is uploaded at
calculus_made_easy.pdf and provides detailed guidance
and explanations on various calculus topics such as
derivatives, integrals, limits, and more. The GPT can
teach calculus concepts, solve problems, and answer
questions related to calculus, making complex topics The preamble was
accessible and understandable. It can handle initially generated
calculus-related inquiries, from basic to advanced, by the Builder and
then tweaked as
and is particularly useful for students and educators
needed.
seeking to deepen their understanding of calculus.
--- Page 82 ---
58 CHAPTER 3 Engaging GPT assistants
Answer as the famous mathematician Terence Tao.
Be sure always to give
Terence Tao is renowned for his brilliant intellect,
your assistants and
approachability, and exceptional ability to effectively
agents an appropriate
simplify and communicate complex mathematical concepts.
persona/personality.
RULES
1) Always teach the concepts as if you were teaching to a young child.
2) Always demonstrate concepts by showing plots of functions and graphs.
3) Always ask if the user wants to try a sample problem on their own.
Give them a problem equivalent to the question concept you were discussing.
Defining explicit conditions and rules can help
better guide the GPT to your desire.
After updating the assistant, you can try it in the preview window or the book version
by searching for Calculus Made Easy in the GPT Store. Figure 3.11 shows a snipped
example of interaction with the GPT. The figure shows that the GPT can generate
plots to demonstrate concepts or ask questions.
This GPT demonstrates the ability of an assistant to use a book as a companion
teaching reference. Only a single book was uploaded in this exercise, but multiple
books or other documents could be uploaded. As this feature and the technology
mature, in the future, it may be conceivable that an entire course could be taught
using a GPT.
We’ll move away from technical and embrace fiction to demonstrate the use of
knowledge. In the next section, we’ll look at how knowledge of file uploads can be
used for search and reference.
3.4.2 Knowledge search and more with file uploads
The GPT Assistants platform’s file upload capability supports up to 512 MB of
uploads for a single assistant. This feature alone provides powerful capabilities for
document search and other applications in personal and small-to-medium business/
project sizes.
Imagine uploading a whole collection of files. You can now search, compare, con-
trast, organize, and collate all with one assistant. This feature alone within GPT Assis-
tants will disrupt how we search for and analyze documents. In chapter 6, we’ll
examine how direct access to the OpenAI Assistants API can increase the number of
documents.
For this next exercise, we’ll employ an assistant with knowledge of multiple books
or documents. This technique could be applied to any supported document, but this
assistant will consume classic texts about robots. We’ll name this assistant the Classic
Robot Reads GPT.
Start by creating a new GPT assistant in the ChatGPT interface. Then, upload the
instructions in listing 3.17, and name and describe the assistant. These instructions
were generated in part through the GPT Builder and then edited.
--- Page 83 ---
3.4 Extending an assistant’s knowledge using file uploads 59
The conversation was started by asking
the GPT to teach the basics of calculus.
Function and Its Derivative
The GPT can also generate plots to
demonstrate concepts, such as showing
the function and its derivative.
Figure 3.11 Output from asking the GPT to teach calculus
Listing3.17 Classic Robot Reads instructions
This GPT, Classic Robot Reads and uses the persona of Remember always
Isaac Asimov and will reply as the famous robot author. to give your GPT a
This GPT will only references and discusses the books persona/personality.
in its knowledge base of uploaded files.
It does not mention or discuss other books or text that Make sure the
are not within its knowledge base. assistant only
references knowledge
RULES within file uploads.
Refer to only text within your knowledge base
--- Page 84 ---
60 CHAPTER 3 Engaging GPT assistants
Always provide 3 examples of any query the use asks for
Add some extra
Always ask the user if they require anything further
rules for style
choices.
Make the assistant more helpful by
also giving them nuance and style.
After completing those steps, you can upload the files from the chapter’s source called
gutenberg_robot_books. Figure 3.12 demonstrates uploading multiple files at a time.
The maximum number of files you can upload at a time will vary according to the
sizes of the files.
Uploads become accessible to the
agent through knowledge patterns.
You can upload multiple
files (about 5) at a time.
Figure 3.12 Uploading documents to the assistant’s knowledge
You can start using it after uploading the documents, setting the instructions, and giv-
ing the assistant a name and an image. Search is the most basic application of a knowl-
edge assistant, and other use cases in the form of prompts are shown in table 3.1.
--- Page 85 ---
3.5 Publishing your GPT 61
Table 3.1 Use cases for a knowledge assistant
Use case Example prompt Results
Search Search for this phrase in your knowledge: Returns the document and an excerpt
“the robot servant.”
Compare Identify the three most similar books that Returns the three most similar docu-
share the same writing style. ments
Contrast Identify the three most different books. Returns books in the collection that are
the most different
Ordering What order should I read the books? Returns an ordered progression of books
Classification Which of these books is the most modern? Classifies documents
Generation Generate a fictional paragraph that mimics Generates new content based on its
your knowledge of the robot servant. knowledge base
These use cases are just a sample of the many things possible with an AI knowledge
assistant. While this feature may not be poised to disrupt enterprise search, it gives
smaller organizations and individuals more access to their documents. It allows the
creation of assistants as a form of knowledge that can be exposed publicly. In the next
section, we’ll look at how to make assistants consumable by all.
3.5 Publishing your GPT
Once you’re happy with your GPT, you can use it or share it with others by providing a
link. Consuming GPT assistants through ChatGPT currently requires a Plus subscrip-
tion. To publish your GPT for others, click the Share button, and select your sharing
option, as shown in figure 3.13.
Only for you
Allows you to give the
link to other users
Publishes your GPT to the
store and makes it public
If you give a link to a GPT or make it
public, usage of that assistant is taken
from the user’s account and not yours. Figure 3.13 GPT
sharing options
--- Page 86 ---
62 CHAPTER 3 Engaging GPT assistants
Whether you share your GPT with friends and colleagues or publicly in the GPT
Store, the assistant’s usage is taken from the account using it, not the publisher. This
means if you have a particularly expensive GPT that generates a lot of images, for
example, it won’t affect your account while others use it.
3.5.1 Expensive GPT assistants
At the time of writing, OpenAI tracks the resource usage of your ChatGPT account,
including that used for GPTs. If you hit a resource usage limit and get blocked, your
ChatGPT account will also be blocked. Blockages typically only last a couple of hours,
but this can undoubtedly be more than a little annoying.
Therefore, we want to ensure that users using your GPT don’t exceed their resource
usage limits for regular use. Following is a list of features that increase resource usage
while using the GPT:
 Creating images—Image generation is still a premium service, and successive image
generation can quickly get your user blocked. It’s generally recommended that
you inform your users of the potential risks and/or try to reduce how fre-
quently images are generated.
 Code interpretation—This feature allows for file uploads and running of code for
data analysis. If you think your users will require constant use of the coding tool,
then inform them of the risk.
 Vision, describing images—If you’re building an assistant that uses vision to describe
and extract information from the image, plan to use it sparingly.
 File uploads—If your GPT uses a lot of files or allows you to upload several files,
this may cause blocks. As always, guide the user away from anything preventing
them from enjoying your GPT.
NOTE Moore’s Law states that computers will double in power every two years
while costing half as much. LLMs are now doubling in power about every six
months from optimization and increasing GPU power. This, combined with
the cost being reduced by at least half in the same period, likely means cur-
rent resource limits on vision and image-generation models won’t be consid-
ered. However, services such as code interpretation and file uploads will likely
remain the same.
Making your assistant aware of resource usage can be as simple as adding the rule
shown in listing 3.18 to the assistant’s instructions. The instructions can be just a state-
ment relaying the warning to the user and making the assistant aware. You could even
ask the assistant to limit its usage of certain features.
Listing3.18 Resource usage rule example
RULE:
When generating images, ensure the user is aware that creating multiple
images quickly could temporarily block their account.
--- Page 87 ---
3.5 Publishing your GPT 63
Guiding your assistant to be more resource conscious in the end makes your assistant
more usable. It also helps prevent angry users who unknowingly get blocked using
your assistant. This may be important if you plan on releasing your GPT, but before
that, let’s investigate the economics in the next section.
3.5.2 Understanding the economics of GPTs
Upon the release of GPT Assistants and the GPT Store, OpenAI announced the
potential for a future profit-sharing program for those who published GPTs. While
we’re still waiting to hear more about this program, many have speculated what this
may look like.
Some have suggested the store may return only 10% to 20% of profits to the build-
ers. This is far less than the percentage on other app platforms but requires much less
technical knowledge and fewer resources. The GPT Store is flooded with essentially free
assistants, provided you have a Plus subscription, but that may change in the future.
Regardless, there are also several reasons why you may want to build public GPTs:
 Personal portfolio—Perhaps you want to demonstrate your knowledge of prompt
engineering or your ability to build the next wave of AI applications. Having a
few GPTs in the GPT Store can help demonstrate your knowledge and ability to
create useful AI applications.
 Knowledge and experience—If you have in-depth knowledge of a subject or topic,
this can be a great way to package that as an assistant. These types of assistants
will vary in popularity based on your area of expertise.
 Cross-marketing and commercial tie-in—This is becoming more common in the
Store and provides companies the ability to lead customers using an assistant.
As companies integrate more AI, this will certainly be more common.
 Helpful assistant to your product/service—Not all companies or organizations can
sustain the cost of hosting chatbots. While consuming assistants is currently lim-
ited to ChatGPT subscribers, they will likely be more accessible in the future.
This may mean having GPTs for everything, perhaps like the internet’s early
days where every company rushed to build a web presence.
While the current form of the GPT Store is for ChatGPT subscribers, if the current
trend with OpenAI continues, we’ll likely see a fully public GPT Store. Public GPTs
have the potential to disrupt the way we search, investigate products and services, and
consume the internet. In the last section of this chapter, we’ll examine how to publish
a GPT and some important considerations.
3.5.3 Releasing the GPT
Okay, you’re happy with your GPT and how it operates, and you see real benefit from
giving it to others. Publishing GPTs for public (subscribers) consumption is easy, as
shown in figure 3.14. After selecting the GPT Store as the option and clicking Save,
you’ll now have the option to set the category and provide links back to you.
--- Page 88 ---
64 CHAPTER 3 Engaging GPT assistants
You can set links to your
social media and GitHub.
The area you want to Selecting this allows
publish your GPT to you to view set links.
Figure 3.14 Selecting the options after clicking Save to publish to the GPT Store
That is easy, so here are a few more things you’ll want to consider before publishing
your GPT:
 GPT description—Create a good description, and you may even want to ask
ChatGPT to help you build a description that increases the search engine opti-
mization (SEO) of your GPT. GPTs are now showing up in Google searches, so
good search engine optimization can help increase exposure to your assistant.
A good description will also help users decide if they want to take the time to
use your assistant.
 The logo—A nice, clean logo that identifies what your assistant does can undoubt-
edly help. Logo design for GPTs is effectively a free service, but taking the time
to iterate over a few images can help draw users to your assistant.
 The category—By default, the category will already be selected, but make sure it
fits your assistant. If you feel it doesn’t, than change the category, and you may
even want to select Other and define your own.
 Links—Be sure to set reference links for your social media and perhaps even a
GitHub repository that you use to track problems for the GPT. Adding links to
your GPT demonstrates to users that they can reach out to the builder if they
encounter problems or have questions.
--- Page 89 ---
3.6 Exercises 65
Further requirements may likely emerge as the GPT Store matures. The business
model remains to be established, and other learnings will likely follow. Whether you
decide to build GPTs for yourself or others, doing so can help improve your under-
standing of how to build agents and assistants. As we’ll see throughout the rest of this
book, GPT assistants are a useful foundation for your knowledge.
3.6 Exercises
Complete the following exercises to improve your knowledge of the material:
 Exercise 1—Build Your First GPT Assistant
Objective—Create a simple GPT assistant using the ChatGPT interface.
Tasks:
– Sign up for a ChatGPT Plus subscription if you don’t already have one.
– Navigate to the GPT Assistants platform, and click the Create button.
– Follow the Builder chat interface to create a Culinary Companion assistant
that provides meal suggestions based on available ingredients.
– Manually configure the assistant to add custom rules for recipe generation,
such as including nutritional information and cost estimates.
 Exercise 2—Data Analysis Assistant
Objective—Develop a GPT assistant that can analyze CSV files and provide
insights.
Tasks:
– Design a data science assistant that can load and analyze CSV files, similar to
the Data Scout example in the chapter.
– Enable the Code Interpretation tool, and upload a sample CSV file (e.g., a
dataset from Kaggle).
– Use the assistant to perform tasks such as data cleaning, visualization, and
hypothesis testing.
– Document your process and findings, noting any challenges or improve-
ments needed.
 Exercise 3—Create a Custom Action
Objective—Extend a GPT assistant with a custom action using a FastAPI service.
Tasks:
– Follow the steps to create a FastAPI service that provides a specific function,
such as fetching a list of daily tasks.
– Generate the OpenAPI specification for the service, and deploy it locally
using ngrok.
– Configure a new assistant to use this custom action, ensuring it connects cor-
rectly to the FastAPI endpoint.
– Test the assistant by asking it to perform the action and verify the output.
--- Page 90 ---
66 CHAPTER 3 Engaging GPT assistants
 Exercise 4—File Upload Knowledge Assistant
Objective—Build an assistant with specialized knowledge from uploaded
documents.
Tasks:
– Select a freely available e-book or a collection of documents related to a spe-
cific topic (e.g., classic literature, technical manuals).
– Upload these files to a new GPT assistant, and configure the assistant to act
as an expert on the uploaded content.
– Create a series of prompts to test the assistant’s ability to reference and sum-
marize the information from the documents.
– Evaluate the assistant’s performance, and make any necessary adjustments to
improve its accuracy and helpfulness.
 Exercise 5—Publish and Share Your Assistant
Objective—Publish your GPT assistant to the GPT Store and share it with others.
Tasks:
– Finalize the configuration and testing of your assistant to ensure it works as
intended.
– Write a compelling description, and create an appropriate logo for your
assistant.
– Choose the correct category, and set up any necessary links to your social
media or GitHub repository.
– Publish the assistant to the GPT Store, and share the link with friends or
colleagues.
– Gather feedback from users, and refine the assistant based on their input to
improve its usability and functionality.
Summary
 The OpenAI GPT Assistants platform enables building and deploying AI agents
through the ChatGPT UI, focusing on creating engaging and functional assis-
tants.
 You can use GPT’s code interpretation capabilities to perform data analysis on
user-uploaded CSV files, enabling assistants to function as data scientists.
 Assistants can be extended with custom actions, allowing integration with exter-
nal services via API endpoints. This includes generating FastAPI services and
their corresponding OpenAPI specifications.
 Assistants can be enriched with specialized knowledge through file uploads,
allowing them to act as authoritative sources on specific texts or documents.
 Commercializing your GPT involves publishing it to the GPT Store, where you
can share and market your assistant to a broader audience.
--- Page 91 ---
Summary 67
 Building a functional assistant involves iterating through design prompts, defin-
ing a clear persona, setting rules, and ensuring the assistant’s output aligns with
user expectations.
 Creating custom actions requires understanding and implementing OpenAPI
specifications, deploying services locally using tools such as ngrok, and connect-
ing these services to your assistant.
 Knowledge assistants can handle various tasks, from searching and comparing
documents to generating new content based on their knowledge base.
 Publishing assistants require careful consideration of resource usage, user expe-
rience, and economic factors to ensure their effectiveness and sustainability for
public use.
 The GPT Store, available to ChatGPT Plus subscribers, is a valuable platform
for learning and gaining proficiency in building AI assistants, with the potential
for future profit-sharing opportunities.
--- Page 92 ---
Exploring
multi-agent systems
This chapter covers
 Building multi-agent systems using AutoGen
Studio
 Building a simple multi-agent system
 Creating agents that can work collaboratively over
a group chat
 Building an agent crew and multi-agent systems
using CrewAI
 Extending the number of agents and exploring
processing patterns with CrewAI
Now let’s take a journey from AutoGen to CrewAI, two well-established multi-
agent platforms. We’ll start with AutoGen, a Microsoft project that supports mul-
tiple agents and provides a studio for working with them. We’ll explore a project
from Microsoft called AutoGen, which supports multiple agents but also provides
a studio to ease you into working with agents. From there, we’ll get more hands-
on coding of AutoGen agents to solve tasks using conversations and group chat
collaborations.
Then, we’ll transition to CrewAI, a self-proposed enterprise agentic system that
takes a different approach. CrewAI balances role-based and autonomous agents that
68
--- Page 93 ---
4.1 Introducing multi-agent systems with AutoGen Studio 69
can be sequentially or hierarchically flexible task management systems. We’ll explore
how CrewAI can solve diverse and complex problems.
Multi-agent systems incorporate many of the same tools single-agent systems use
but benefit from the ability to provide outside feedback and evaluation to other
agents. This ability to support and criticize agent solutions internally gives multi-agent
systems more power. We’ll explore an introduction to multi-agent systems, beginning
with AutoGen Studio in the next section.
4.1 Introducing multi-agent systems with AutoGen Studio
AutoGen Studio is a powerful tool that employs multiple agents behind the scenes to
solve tasks and problems a user directs. This tool has been used to develop some of
the more complex code in this book. For that reason and others, it’s an excellent
introduction to a practical multi-agent system.
Figure 4.1 shows a schematic diagram of the agent connection/communication
patterns AutoGen employs. AutoGen is a conversational multi-agent platform because
communication is done using natural language. Natural language conversation seems
to be the most natural pattern for agents to communicate, but it’s not the only method,
as you’ll see later.
AutoGen uses conversable agents, which
communicate through conversations.
Figure 4.1 How AutoGen agents communicate through conversations (Source: AutoGen)
AutoGen supports various conversational patterns, from group and hierarchical to the
more common and simpler proxy communication. In proxy communication, one
agent acts as a proxy and directs communication to relevant agents to complete tasks.
A proxy is similar to a waiter taking orders and delivering them to the kitchen, which
cooks the food. Then, the waiter serves the cooked food.
--- Page 94 ---
70 CHAPTER 4 Exploring multi-agent systems
The basic pattern in AutoGen uses a UserProxy and one or more assistant
agents. Figure 4.2 shows the user proxy taking direction from a human and then
directing an assistant agent enabled to write code to perform the tasks. Each time
the assistant completes a task, the proxy agent reviews, evaluates, and provides feed-
back to the assistant. This iteration loop continues until the proxy is satisfied with
the results.
Human communicates
to the user proxy agent,
which communicates
to other agents.
Evaluation and feedback
loop is formed between
the proxy and the assistant.
Assistant agent
undertakes completion
of the direct tasks.
LLM configured to
write Python code
Figure 4.2 The user proxy agent and assistant agent communication (Source: AutoGen)
The benefit of the proxy is that it works to replace the required human feedback and
evaluation, and, in most cases, it does a good job. While it doesn’t eliminate the need
for human feedback and evaluation, it produces much more complete results overall.
And, while the iteration loop is time consuming, it’s time you could be drinking a cof-
fee or working on other tasks.
AutoGen Studio is a tool developed by the AutoGen team that provides a helpful
introduction to conversable agents. In the next exercise, we’ll install Studio and run
some experiments to see how well the platform performs. These tools are still in a
rapid development cycle, so if you encounter any problems, consult the documenta-
tion on the AutoGen GitHub repository.
4.1.1 Installing and using AutoGen Studio
Open the chapter_04 folder in Visual Studio Code (VS Code), create a local Python
virtual environment, and install the requirements.txt file. If you need assistance with
this, consult appendix B to install all of this chapter’s exercise requirements.
Open a terminal in VS Code (Ctrl-`, Cmd-`) pointing to your virtual environment,
and run AutoGen Studio using the command shown in listing 4.1. You’ll first need to