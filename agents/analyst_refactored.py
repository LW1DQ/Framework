"""
Agente Analista - Versi√≥n Refactorizada

Responsable de analizar resultados de simulaciones y proponer optimizaciones.
Refactorizado para mejor mantenibilidad y separaci√≥n de responsabilidades.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from typing import Dict, List, Any
from langchain_ollama import ChatOllama

from config.settings import OLLAMA_BASE_URL, MODEL_REASONING, SIMULATIONS_DIR
from utils.state import AgentState, add_audit_entry
from utils.logging_utils import update_agent_status, log_message
from utils.dependency_injection import get_reasoning_llm
from utils.retry_patterns import resilient_llm_call
from agents.analysis import MetricsAnalyzer, ReportGenerator
from utils.statistical_tests import (
    t_test_two_samples,
    anova_test,
    calculate_confidence_interval,
    calculate_all_confidence_intervals,
    generate_statistical_report
)


class AnalysisEngine:
    """
    Motor de an√°lisis que coordina todos los componentes de an√°lisis
    """
    
    def __init__(self):
        """Inicializa el motor de an√°lisis con sus componentes"""
        self.metrics_analyzer = MetricsAnalyzer()
        self.report_generator = ReportGenerator()
        self.llm = get_reasoning_llm()
    
    def perform_comprehensive_analysis(self, 
                                     simulation_logs: str,
                                     pcap_files: List[str],
                                     comparison_metrics: List[str] = None) -> Dict[str, Any]:
        """
        Realiza an√°lisis comprensivo de resultados de simulaci√≥n
        
        Args:
            simulation_logs: Ruta a logs de simulaci√≥n
            pcap_files: Lista de archivos PCAP
            comparison_metrics: M√©tricas para comparaci√≥n estad√≠stica
            
        Returns:
            Diccionario con an√°lisis completo
        """
        analysis_results = {
            'metrics_analysis': {},
            'trace_analysis': {},
            'statistical_analysis': {},
            'performance_summary': {},
            'recommendations': []
        }
        
        try:
            # 1. An√°lisis de m√©tricas desde logs
            if simulation_logs and Path(simulation_logs).exists():
                metrics_df = self.metrics_analyzer.parse_flowmonitor_xml(simulation_logs)
                
                if not metrics_df.empty:
                    # Calcular estad√≠sticas resumidas
                    metrics_summary = self.metrics_analyzer.calculate_summary_statistics(metrics_df)
                    analysis_results['metrics_analysis'] = {
                        'raw_data': metrics_df.to_dict('records')[:100],  # Limitar para no sobrecargar
                        'summary': metrics_summary,
                        'data_points': len(metrics_df)
                    }
                    
                    # Detectar anomal√≠as
                    anomalies = self.metrics_analyzer.detect_anomalies(metrics_df)
                    analysis_results['anomalies'] = anomalies
                    
                    # Calcular m√©tricas clave para el estado
                    analysis_results['performance_summary'] = self._extract_key_metrics(metrics_summary)
            
            # 2. An√°lisis de trazas PCAP
            if pcap_files:
                trace_analysis = self.metrics_analyzer.analyze_pcap_traces(pcap_files)
                analysis_results['trace_analysis'] = trace_analysis
            
            # 3. An√°lisis estad√≠stico avanzado
            if comparison_metrics and analysis_results['metrics_analysis']:
                statistical_results = self._perform_statistical_analysis(
                    analysis_results['metrics_analysis']['summary'],
                    comparison_metrics
                )
                analysis_results['statistical_analysis'] = statistical_results
            
            # 4. Generar recomendaciones con LLM
            analysis_results['recommendations'] = self._generate_llm_recommendations(analysis_results)
            
            # 5. Generar reporte completo
            comprehensive_report = self.report_generator.generate_comprehensive_report(
                metrics_summary=analysis_results.get('metrics_analysis', {}).get('summary', {}),
                performance_analysis=analysis_results.get('performance_summary', {}),
                trace_analysis=analysis_results.get('trace_analysis', {}),
                anomalies=analysis_results.get('anomalies', {}),
                simulation_context={
                    'simulation_logs': simulation_logs,
                    'pcap_files': pcap_files,
                    'task': 'Simulation Analysis'
                }
            )
            
            # Guardar reporte
            report_path = self._save_analysis_report(comprehensive_report)
            analysis_results['report_path'] = report_path
            
            log_message("Analyst", "An√°lisis comprensivo completado exitosamente")
            
            return analysis_results
            
        except Exception as e:
            error_msg = f"Error en an√°lisis comprensivo: {str(e)}"
            log_message("Analyst", error_msg, level="ERROR")
            analysis_results['error'] = error_msg
            return analysis_results
    
    def _extract_key_metrics(self, metrics_summary: Dict) -> Dict:
        """Extrae m√©tricas clave para el estado del agente"""
        key_metrics = {}
        
        # M√©tricas principales
        if 'pdr' in metrics_summary:
            key_metrics['avg_pdr'] = metrics_summary['pdr'].get('mean', 0)
            key_metrics['std_pdr'] = metrics_summary['pdr'].get('std', 0)
        
        if 'avg_delay_ms' in metrics_summary:
            key_metrics['avg_delay'] = metrics_summary['avg_delay_ms'].get('mean', 0)
            key_metrics['std_delay'] = metrics_summary['avg_delay_ms'].get('std', 0)
        
        if 'throughput_mbps' in metrics_summary:
            key_metrics['avg_throughput'] = metrics_summary['throughput_mbps'].get('mean', 0)
            key_metrics['std_throughput'] = metrics_summary['throughput_mbps'].get('std', 0)
        
        if 'packet_loss_rate' in metrics_summary:
            key_metrics['packet_loss_rate'] = metrics_summary['packet_loss_rate'].get('mean', 0)
        
        if 'qos_score' in metrics_summary:
            key_metrics['qos_score'] = metrics_summary['qos_score'].get('mean', 0)
        
        # Calcular m√©tricas adicionales
        if 'avg_pdr' in key_metrics:
            key_metrics['success_rate'] = key_metrics['avg_pdr']
        
        return key_metrics
    
    def _perform_statistical_analysis(self, metrics_summary: Dict, comparison_metrics: List[str]) -> Dict:
        """Realiza an√°lisis estad√≠stico avanzado"""
        statistical_results = {}
        
        try:
            # Calcular intervalos de confianza para todas las m√©tricas
            confidence_intervals = calculate_all_confidence_intervals(metrics_summary)
            statistical_results['confidence_intervals'] = confidence_intervals
            
            # Realizar tests estad√≠sticos si hay m√∫ltiples m√©tricas
            if len(comparison_metrics) >= 2:
                for metric in comparison_metrics:
                    if metric in metrics_summary:
                        stats = metrics_summary[metric]
                        # Simular test de hip√≥tesis (en producci√≥n con datos reales)
                        if stats['count'] >= 2:
                            # Test t contra valores de referencia
                            t_statistic, p_value = self._simulate_hypothesis_test(stats)
                            statistical_results[f'{metric}_test'] = {
                                't_statistic': t_statistic,
                                'p_value': p_value,
                                'significant': p_value < 0.05
                            }
            
        except Exception as e:
            log_message("Analyst", f"Error en an√°lisis estad√≠stico: {e}", level="WARNING")
            statistical_results['error'] = str(e)
        
        return statistical_results
    
    def _simulate_hypothesis_test(self, stats: Dict) -> tuple:
        """Simula test de hip√≥tesis (placeholder para implementaci√≥n real)"""
        # Implementaci√≥n simplificada - en producci√≥n usar datos reales
        mean = stats.get('mean', 0)
        std = stats.get('std', 1)
        n = stats.get('count', 10)
        
        # Test t contra valor de referencia hipot√©tico
        reference_value = 80 if 'pdr' in str(stats) else 50  # Referencia seg√∫n m√©trica
        
        import scipy.stats as stats_lib
        t_statistic = (mean - reference_value) / (std / (n ** 0.5))
        p_value = 2 * (1 - stats_lib.t.cdf(abs(t_statistic), n - 1))
        
        return t_statistic, p_value
    
    @resilient_llm_call(max_retries=2, timeout=60)
    def _generate_llm_recommendations(self, analysis_results: Dict) -> List[str]:
        """Genera recomendaciones usando LLM"""
        prompt = f"""
        Analiza los siguientes resultados de simulaci√≥n de red y genera recomendaciones espec√≠ficas:
        
        M√âTRICAS DE RENDIMIENTO:
        {self._format_metrics_for_llm(analysis_results.get('metrics_analysis', {}).get('summary', {}))}
        
        ANOMAL√çAS DETECTADAS:
        {self._format_anomalies_for_llm(analysis_results.get('anomalies', {}))}
        
        AN√ÅLISIS DE TRAZAS:
        {self._format_traces_for_llm(analysis_results.get('trace_analysis', {}))}
        
        Genera 3-5 recomendaciones espec√≠ficas y accionables para mejorar el rendimiento.
        Clasifica cada recomendaci√≥n como 'alta', 'media' o 'baja' prioridad.
        
        Responde en formato JSON:
        {{
            "recommendations": [
                {{
                    "description": "Descripci√≥n de la recomendaci√≥n",
                    "priority": "alta|media|baja",
                    "expected_impact": "Impacto esperado",
                    "implementation_complexity": "complejidad"
                }}
            ]
        }}
        """
        
        response = self.llm.invoke(prompt)
        
        # Parsear respuesta JSON
        try:
            import json
            result = json.loads(response.content)
            return [rec['description'] for rec in result.get('recommendations', [])]
        except:
            # Fallback a texto plano
            return [
                "Optimizar par√°metros del protocolo basados en resultados",
                "Implementar control de congesti√≥n si es necesario",
                "Revisar configuraci√≥n de la red para mejorar latencia"
            ]
    
    def _format_metrics_for_llm(self, metrics_summary: Dict) -> str:
        """Formatea m√©tricas para el prompt del LLM"""
        if not metrics_summary:
            return "No hay m√©tricas disponibles"
        
        formatted = []
        for metric, stats in metrics_summary.items():
            formatted.append(f"- {metric}: {stats.get('mean', 0):.2f} ¬± {stats.get('std', 0):.2f}")
        
        return "\n".join(formatted)
    
    def _format_anomalies_for_llm(self, anomalies: Dict) -> str:
        """Formatea anomal√≠as para el prompt del LLM"""
        if not anomalies:
            return "No se detectaron anomal√≠as"
        
        formatted = []
        
        outliers = anomalies.get('statistical_outliers', {})
        for metric, outlier_data in outliers.items():
            formatted.append(f"- Outliers en {metric}: {outlier_data.get('count', 0)} valores")
        
        issues = anomalies.get('performance_issues', {})
        for issue_type, issue_data in issues.items():
            formatted.append(f"- {issue_type}: {issue_data.get('count', 0)} flujos afectados")
        
        return "\n".join(formatted) if formatted else "No se detectaron problemas de rendimiento"
    
    def _format_traces_for_llm(self, trace_analysis: Dict) -> str:
        """Formatea an√°lisis de trazas para el prompt del LLM"""
        if not trace_analysis:
            return "No hay an√°lisis de trazas disponible"
        
        formatted = []
        
        if trace_analysis.get('total_packets'):
            formatted.append(f"- Total de paquetes: {trace_analysis['total_packets']}")
        
        protocols = trace_analysis.get('protocols', {})
        if protocols:
            protocol_str = ", ".join([f"{p}: {c}" for p, c in protocols.items()])
            formatted.append(f"- Distribuci√≥n de protocolos: {protocol_str}")
        
        return "\n".join(formatted) if formatted else "An√°lisis de trazas vac√≠o"
    
    def _save_analysis_report(self, report: Dict) -> str:
        """Guarda el reporte de an√°lisis"""
        timestamp = str(int(Path().resolve().name))
        report_path = SIMULATIONS_DIR / f"analysis_report_{timestamp}.json"
        
        success = self.report_generator.save_report(report, str(report_path))
        
        if success:
            log_message("Analyst", f"Reporte guardado en: {report_path}")
            return str(report_path)
        else:
            return ""


def analyst_node(state: AgentState) -> AgentState:
    """
    Nodo del analista para LangGraph
    
    Args:
        state: Estado actual del sistema
        
    Returns:
        Estado actualizado con resultados del an√°lisis
    """
    try:
        update_agent_status("Analyst", "processing", "Analizando resultados de simulaci√≥n")
        log_message("Analyst", "Iniciando an√°lisis de simulaci√≥n")
        
        # Verificar que existan datos de simulaci√≥n
        simulation_logs = state.get('simulation_logs', '')
        pcap_files = state.get('pcap_files', [])
        comparison_metrics = state.get('comparison_metrics', [])
        
        if not simulation_logs and not pcap_files:
            error_msg = "No se encontraron datos de simulaci√≥n para analizar"
            log_message("Analyst", error_msg, level="ERROR")
            return {
                'errors': [error_msg],
                **add_audit_entry(state, "analyst", "failed", {"error": error_msg})
            }
        
        print(f"\nüìä ANALIZANDO RESULTADOS")
        print(f"{'='*50}")
        print(f"Logs de simulaci√≥n: {simulation_logs or 'No disponibles'}")
        print(f"Archivos PCAP: {len(pcap_files)}")
        print(f"M√©tricas de comparaci√≥n: {len(comparison_metrics)}")
        
        # Crear motor de an√°lisis
        analysis_engine = AnalysisEngine()
        
        # Realizar an√°lisis comprensivo
        analysis_results = analysis_engine.perform_comprehensive_analysis(
            simulation_logs=simulation_logs,
            pcap_files=pcap_files,
            comparison_metrics=comparison_metrics
        )
        
        # Verificar si hubo errores
        if 'error' in analysis_results:
            return {
                'errors': [analysis_results['error']],
                **add_audit_entry(state, "analyst", "failed", {"error": analysis_results['error']})
            }
        
        # Extraer resultados para el estado
        performance_summary = analysis_results.get('performance_summary', {})
        metrics_analysis = analysis_results.get('metrics_analysis', {})
        trace_analysis = analysis_results.get('trace_analysis', {})
        statistical_analysis = analysis_results.get('statistical_analysis', {})
        
        print(f"\n‚úÖ AN√ÅLISIS COMPLETADO")
        print(f"{'='*50}")
        print(f"M√©tricas analizadas: {len(performance_summary)}")
        print(f"Puntos de datos: {metrics_analysis.get('data_points', 0)}")
        print(f"Anomal√≠as detectadas: {len(analysis_results.get('anomalies', {}).get('statistical_outliers', {}))}")
        print(f"Recomendaciones: {len(analysis_results.get('recommendations', []))}")
        
        if analysis_results.get('report_path'):
            print(f"Reporte guardado: {Path(analysis_results['report_path']).name}")
        
        log_message("Analyst", "An√°lisis completado exitosamente")
        update_agent_status("Analyst", "completed", "An√°lisis finalizado")
        
        # Construir estado de retorno
        return_state = {
            'analysis_results': {
                'performance_summary': performance_summary,
                'metrics_analysis': metrics_analysis,
                'trace_analysis': trace_analysis,
                'statistical_analysis': statistical_analysis,
                'anomalies': analysis_results.get('anomalies', {}),
                'recommendations': analysis_results.get('recommendations', []),
                'report_path': analysis_results.get('report_path', '')
            },
            'metrics': performance_summary,  # Para compatibilidad con otros agentes
            'messages': [
                f'An√°lisis completado: {len(performance_summary)} m√©tricas procesadas',
                f'Datos analizados: {metrics_analysis.get("data_points", 0)} puntos',
                f'Recomendaciones generadas: {len(analysis_results.get("recommendations", []))}'
            ],
            **add_audit_entry(state, "analyst", "analysis_completed", {
                'metrics_processed': len(performance_summary),
                'data_points': metrics_analysis.get('data_points', 0),
                'anomalies_found': len(analysis_results.get('anomalies', {}).get('statistical_outliers', {})),
                'recommendations_count': len(analysis_results.get('recommendations', [])),
                'report_generated': bool(analysis_results.get('report_path'))
            })
        }
        
        # A√±adir resultados estad√≠sticos si existen
        if statistical_analysis:
            return_state['statistical_results'] = statistical_analysis
        
        return return_state
        
    except Exception as e:
        error_msg = f"Error en agente analista: {str(e)}"
        log_message("Analyst", error_msg, level="ERROR")
        update_agent_status("Analyst", "failed", error_msg)
        
        return {
            'errors': [error_msg],
            **add_audit_entry(state, "analyst", "failed", {"error": error_msg})
        }


if __name__ == "__main__":
    # Prueba del agente refactorizado
    from utils.state import create_initial_state
    
    test_state = create_initial_state("An√°lisis de simulaci√≥n AODV")
    
    # Simular datos de simulaci√≥n
    test_state['simulation_logs'] = "/tmp/test_simulation.xml"
    test_state['pcap_files'] = ["/tmp/test.pcap"]
    test_state['comparison_metrics'] = ['pdr', 'delay', 'throughput']
    
    # Crear archivo de prueba
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False) as f:
        f.write('''<?xml version="1.0" encoding="UTF-8"?>
        <FlowMonitor>
            <FlowStats>
                <Flow flowId="1" txPackets="1000" rxPackets="850" txBytes="1000000" rxBytes="850000" delaySum="85000000ns" jitterSum="1000000ns" lostPackets="150"/>
                <Flow flowId="2" txPackets="800" rxPackets="720" txBytes="800000" rxBytes="720000" delaySum="72000000ns" jitterSum="800000ns" lostPackets="80"/>
            </FlowStats>
        </FlowMonitor>''')
        test_state['simulation_logs'] = f.name
    
    result = analyst_node(test_state)
    
    print("\n" + "="*80)
    print("RESULTADO DE PRUEBA - ANALISTA REFACTORIZADO")
    print("="*80)
    print(f"An√°lisis completado: {bool(result.get('analysis_results'))}")
    print(f"M√©tricas procesadas: {len(result.get('metrics', {}))}")
    print(f"Recomendaciones: {len(result.get('analysis_results', {}).get('recommendations', []))}")
    
    # Verificar componentes modulares
    engine = AnalysisEngine()
    print(f"\nComponentes inicializados:")
    print(f"‚Ä¢ Metrics Analyzer: ‚úì")
    print(f"‚Ä¢ Report Generator: ‚úì")
    print(f"‚Ä¢ LLM Client: ‚úì")
    print(f"‚Ä¢ Statistical Analysis: ‚úì")