--- Página 1 ---
Manual de Implementación Técnica:
Sistema Multi-Agente para Optimización
de Redes en NS-3
Versión del Documento: 1.0
Objetivo: Desplegar una orquestación de agentes autónomos (A2A) para investigar, simular y
optimizar protocolos de enrutamiento en Smart Cities utilizando hardware local y
herramientas Open Source.
1. Requisitos Previos y Arquitectura de Hardware
Para cumplir con el requisito de "coste cero", utilizaremos tu propio hardware como servidor
de inferencia o Google Colab como respaldo.
1.1 Stack Tecnológico
● Motor de IA: Ollama (Inferencia local de LLMs).
● Orquestación: LangGraph (Gestión de estado y flujo cíclico).
● Simulador: NS-3 (v3.40+) con Python bindings habilitados.
● Interfaz IA-Red: ns3-ai (Memoria compartida para alta velocidad) o ns3-gym.
● Base de Datos Vectorial: ChromaDB (Persistencia de documentos académicos).
2. Fase 1: Configuración del Motor de Inteligencia
(Ollama)
El primer paso es levantar el servidor que procesará el razonamiento de los agentes.

--- Página 2 ---
1. Instalación:
○ Linux/Mac: curl -fsSL https://ollama.com/install.sh | sh
○ Windows: Descargar el instalador oficial de ollama.com.
2. Descarga de Modelos Especializados:
Necesitas modelos distintos para razonar y para programar. Ejecuta en tu terminal:
Bash
# Modelo para el Supervisor y Redactor (Razonamiento general)
ollama pull llama3
# Modelo para el Agente Programador (Especialista en código)
# DeepSeek Coder v2 es actualmente el mejor open source para código
ollama pull deepseek-coder
# Modelo para Embeddings (Búsqueda de papers)
ollama pull nomic-embed-text
3. Verificación:
Asegúrate de que el servidor escucha en el puerto 11434:
curl http://localhost:11434/api/tags
3. Fase 2: Entorno de Simulación NS-3
Esta es la parte crítica. Los agentes escribirán scripts en Python que llaman a las librerías de
C++ de NS-3.
3.1 Compilación de NS-3 con Python Bindings
No uses apt install ns3 (suele estar desactualizado). Compila desde el código fuente para
tener soporte de IA.
Bash
# 1. Instalar dependencias (Ubuntu/Debian)
sudo apt install g++ python3 python3-dev pkg-config sqlite3 cmake

--- Página 3 ---
# 2. Descargar NS-3 (Versión recomendada para estabilidad con ns3-ai)
wget https://www.nsnam.org/releases/ns-allinone-3.40.tar.bz2
tar xjf ns-allinone-3.40.tar.bz2
cd ns-allinone-3.40/ns-3.40
# 3. Configurar habilitando Python
./ns3 configure --enable-python-bindings --enable-examples
# 4. Compilar (Esto tardará varios minutos)
./ns3 build
3.2 Instalación del Módulo de IA (ns3-ai)
Para que tus agentes puedan proponer redes neuronales que cambien el enrutamiento en
tiempo real:
Bash
cd contrib
git clone https://github.com/hust-diangroup/ns3-ai.git
cd..
./ns3 configure --enable-python-bindings
./ns3 build
pip install./contrib/ns3-ai/python/ns3-ai # Instala la librería puente en tu Python
4. Fase 3: Desarrollo del Sistema de Agentes (Python +
LangGraph)
Crea un proyecto Python (entorno virtual recomendado).
pip install langgraph langchain langchain-community langchain-ollama chromadb pandas

--- Página 4 ---
matplotlib semanticscholar
4.1 Definición del Estado Global (state.py)
Este es el "cerebro compartido" que pasa de un agente a otro.
Python
from typing import TypedDict, List, Annotated
import operator
class AgentState(TypedDict):
task: str # El objetivo actual (ej: "Simular AODV vs GNN")
research_notes: List[str] # Resúmenes de papers encontrados
code_snippet: str # El código Python para NS-3 actual
simulation_logs: str # Rutas a los archivos XML/CSV generados
analysis_report: str # Texto del análisis final
errors: List[str] # Errores de compilación/ejecución para corregir
iteration_count: int # Para evitar bucles infinitos
messages: Annotated[List[str], operator.add] # Historial de chat
4.2 El Agente Investigador (agents/researcher.py)
Utiliza la API pública de Semantic Scholar.
Python
from langchain_community.tools.semanticscholar.tool import SemanticScholarQueryRun
from langchain_ollama import ChatOllama

--- Página 5 ---
def research_node(state):
print("--- AGENTE INVESTIGADOR: Buscando papers ---")
topic = state['task']
# Herramienta de búsqueda
tool = SemanticScholarQueryRun()
results = tool.run(f"routing protocols optimization {topic} ns-3")
# Sintetizar con LLM local
llm = ChatOllama(model="llama3", temperature=0)
synthesis = llm.invoke(f"Resume los hallazgos clave para implementar en NS-3 basados en:
{results}")
return {"research_notes": [synthesis.content]}
4.3 El Agente Programador (agents/coder.py)
Este agente debe ser instruido para usar los Python bindings de NS-3.
Python
def coder_node(state):
print("--- AGENTE PROGRAMADOR: Escribiendo script NS-3 ---")
llm = ChatOllama(model="deepseek-coder", temperature=0.1)
prompt = f"""
Eres un experto en NS-3 con Python. Genera un script completo de simulación.
Objetivo: {state['task']}
Consideraciones de investigación: {state['research_notes']}
REGLAS:
1. Usa 'import ns.applications', 'import ns.core', etc.
2. Configura FlowMonitor para exportar estadísticas a 'resultados.xml'.
3. NO uses C++, solo Python.
4. Devuelve SOLO el código, sin markdown.
"""
if state.get('errors'):

--- Página 6 ---
prompt += f"\n\nCorrige el código anterior basándote en este error: {state['errors'][-1]}"
code = llm.invoke(prompt).content
return {"code_snippet": code, "iteration_count": state['iteration_count'] + 1}
4.4 El Agente Ejecutor de Simulación (agents/simulator.py)
Este agente ejecuta el código y captura errores reales del sistema.
Python
import subprocess
import os
def simulation_node(state):
print("--- AGENTE SIMULADOR: Ejecutando NS-3 ---")
code = state['code_snippet']
# Guardar el código en un archivo temporal dentro del directorio de NS-3
filename = "scratch/tesis_sim.py"
with open(f"ns-allinone-3.40/ns-3.40/{filename}", "w") as f:
f.write(code)
# Ejecutar usando el wrapper de NS-3
try:
# Comando:./ns3 run scratch/tesis_sim.py
result = subprocess.run(
["./ns3", "run", filename],
cwd="ns-allinone-3.40/ns-3.40",
capture_output=True,
text=True,
timeout=600 # 10 minutos máximo
)
if result.returncode!= 0:
return {"errors": [result.stderr]}

--- Página 7 ---
else:
return {"simulation_logs": "ns-allinone-3.40/ns-3.40/resultados.xml", "errors":}
except Exception as e:
return {"errors": [str(e)]}
5. Fase 4: La Bitácora y Orquestación (El Supervisor)
Aquí unimos todo usando LangGraph y añadimos la persistencia (tu bitácora).
Python
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.sqlite import SqliteSaver
import sqlite3
# 1. Definir el Grafo
workflow = StateGraph(AgentState)
# 2. Añadir Nodos
workflow.add_node("investigador", research_node)
workflow.add_node("programador", coder_node)
workflow.add_node("simulador", simulation_node)
# (Añadirías aquí el analista y el evaluador de protocolos)
# 3. Definir el Flujo (Edges)
workflow.set_entry_point("investigador")
workflow.add_edge("investigador", "programador")
workflow.add_edge("programador", "simulador")
# 4. Lógica Condicional (Router)
def check_simulation(state):
if state.get('errors'):
if state['iteration_count'] > 3:
return END # Evitar bucle infinito, pedir ayuda humana
return "programador" # Volver a intentar corregir

--- Página 8 ---
return END # O pasar al agente analista
workflow.add_conditional_edges(
"simulador",
check_simulation,
{
"programador": "programador",
END: END
}
)
# 5. Configurar la Bitácora (Persistencia)
# Esto guarda automáticamente cada paso en un archivo local
memory = SqliteSaver(sqlite3.connect("bitacora_tesis.db"))
# 6. Compilar
app = workflow.compile(checkpointer=memory)
6. Ejecución del Sistema
Para iniciar una nueva investigación, ejecutas un script principal:
Python
# main.py
from uuid import uuid4
thread_id = str(uuid4()) # Identificador único de este experimento
config = {"configurable": {"thread_id": thread_id}}
inputs = {
"task": "Comparar latencia entre AODV y OLSR en una red vehicular con 50 nodos",
"iteration_count": 0
}
for event in app.stream(inputs, config=config):

--- Página 9 ---
for key, value in event.items():
print(f"Terminado paso: {key}")
Cómo consultar la Bitácora
Dado que usamos SqliteSaver, puedes crear un script simple para exportar tu progreso a
Markdown cuando lo necesites:
Python
# export_logbook.py
# Script para leer la base de datos y generar un reporte legible
import sqlite3
import pandas as pd
conn = sqlite3.connect("bitacora_tesis.db")
# Las tablas de LangGraph suelen ser 'checkpoints' y 'writes'
df = pd.read_sql_query("SELECT * FROM checkpoints", conn)
df.to_markdown("bitacora_completa.md")
7. Integración de Redes Neuronales (El "Toque
Doctoral")
Para la parte de tu tesis sobre "proponer alternativas con redes neuronales", el Agente
Programador debe generar dos archivos:
1. Script de Entrenamiento (Python puro): Usa PyTorch/TensorFlow. Recibe estado de
red (matriz de tráfico) y devuelve pesos o decisiones de enrutamiento.
2. Script de NS-3 con ns3-ai:
○ Debe incluir la configuración de memoria compartida:
Python
# Ejemplo conceptual dentro del script generado para NS-3
import ns.ns3ai

--- Página 10 ---
mem = ns.ns3ai.SharedMemoryPool(1234) # ID de memoria compartida
○ El Agente Simulador deberá ejecutar ambos scripts en paralelo (usando
subprocess.Popen para el script de IA y luego lanzando NS-3).
8. Anexo: Opción Nube (Si falta RAM)
Si tu simulación es muy pesada (cientos de nodos en Smart City), usa Google Colab como
"trabajador":
1. Abre un Notebook en Colab.
2. Instala Ollama y pyngrok.
3. Ejecuta:
!ollama serve &
from pyngrok import ngrok
# Tu token de ngrok
public_url = ngrok.connect(11434)
print(f"Ollama URL: {public_url}")
```
4. En tu script local coder_node, cambia la URL base de LangChain para que apunte a esa
URL de ngrok. Así, tu PC orquesta, pero Colab "piensa".
